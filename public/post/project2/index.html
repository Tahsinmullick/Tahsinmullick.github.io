<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Project 2: Deeplearning applied to human activity dataset with multimodal timeseries data | Tahsin Mullick</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Deep learning applied to multimodal time series data">
    <meta name="generator" content="Hugo 0.92.2" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="https://Tahsinmullick.github.io/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="Project 2: Deeplearning applied to human activity dataset with multimodal timeseries data" />
<meta property="og:description" content="Deep learning applied to multimodal time series data" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Tahsinmullick.github.io/post/project2/" /><meta property="article:section" content="post" />

<meta property="og:site_name" content="Tahsin Mullick" />

<meta itemprop="name" content="Project 2: Deeplearning applied to human activity dataset with multimodal timeseries data">
<meta itemprop="description" content="Deep learning applied to multimodal time series data">

<meta itemprop="wordCount" content="1261">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Project 2: Deeplearning applied to human activity dataset with multimodal timeseries data"/>
<meta name="twitter:description" content="Deep learning applied to multimodal time series data"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://Tahsinmullick.github.io/images/project2_front.png');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://Tahsinmullick.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Tahsin Mullick
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://Tahsinmullick.github.io/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://Tahsinmullick.github.io/post/" title="Projects page">
              Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://Tahsinmullick.github.io/publications/" title="Publications page">
              Publications
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://Tahsinmullick.github.io/research/" title="Research page">
              Research
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
    <a href="https://twitter.com/tahsinmullick" target="_blank" class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel="noopener" aria-label="follow on Twitter——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>
    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Project 2: Deeplearning applied to human activity dataset with multimodal timeseries data</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Deep learning applied to multimodal time series data
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
      
      <a href="https://twitter.com/share?url=https://Tahsinmullick.github.io/post/project2/&amp;text=Project%202:%20Deeplearning%20applied%20to%20human%20activity%20dataset%20with%20multimodal%20timeseries%20data" class="ananke-social-link twitter no-underline" aria-label="share on Twitter">
        
        <span class="icon"> <svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
        
      </a>
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Project 2: Deeplearning applied to human activity dataset with multimodal timeseries data</h1>
      
      
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><img src="https://Tahsinmullick.github.io/images/project2_front.png" alt=""></p>
<p>The dataset I chose to work with is a popular human activity recognition data that has been used in a lot of publications as means to test the performance of algorithms. It is one of the few open-sourced datasets available online.</p>
<p>This section is divided into subsections that explain the dataset used, 
dataset preprocessing which includes addressing missing data and over-fitting strategy utilized, this is followed by baseline modeling with classical machine learning algorithms, modeling with deep learning algorithm and finally results and discussion.</p>
<p>Dataset description:
The UCI Activity Recognition Using Smartphones dataset is the dataset that will be used in this study. The data consisted of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities:</p>
<ol>
<li>Walking,</li>
<li>Walking Upstairs,</li>
<li>Walking Downstairs,</li>
<li>Sitting,</li>
<li>Standing and</li>
<li>Laying</li>
</ol>
<p>while wearing a smartphone on their waist.
This is a multimodal dataset that draws data from triaxial accelerometer readings and triaxial gyroscope readings. The dataset holds 561 feature vector with time and frequency domain variables along with activity label and subject identifier.  Observations were recorded at 50Hz. Each subject performed the sequence of activities twice, once with the device on their left hand side and once with the device on the right hand side.</p>
<p>The data had already a few preprocessing steps done which included preprocessing accelerometer and gyroscope using noise filters. Splitting data into fixed window of 2.56 seconds (128 data points) with 50% overlap. And splitting of accelerometer data into gravitational and body motion components.</p>
<p>The dataset is split into a 70:30 ratio of train and test based on data for subjects. Early experiment results based on the work by the Anguita et al resulted in 89% on test dataset with an unmodified SVM.</p>
<h4 id="data-preprocessing---missing-data-and-overfitting-strategy">Data preprocessing - missing data and overfitting strategy:</h4>
<p>The missing data was handled keeping in mind that this is a time series data set. Utility of simple imputation techniques will result in bias in the model. Therefore, the application of a forward moving average window was used to fill in the missing values. The features were treated as univariates to address this. If a feature had more than 50% data missing it was dropped completely. However, this was not the case in this dataset.</p>
<p>The problem of overfitting is dealt in the deep learning part of the code. Where I incorporated dropout to mitigate overfitting. Dropout of 0.5 is used in both the networks that were created in this study.</p>
<p>These two were the main strategies employed to treat missing data and overfitting of models.Apart from these preprocessing steps, I also checked for class imbalance for both training dataset and testing datasets. The results of which are presented in following table.</p>
<p><img src="https://Tahsinmullick.github.io/images/project2_trainingdataset.png" alt=""></p>
<p>The training dataset does not show any major class imbalance. The case was similar for the test dataset as shown in the Table 2.
<img src="https://Tahsinmullick.github.io/images/project2_testdataset.png" alt=""></p>
<h4 id="exploratory-data-analysis">Exploratory data analysis:</h4>
<p>EDA was performed to understand the structure of the data and the type of activity in the dataset. I performed three main plots to get a sense of the data. The first plot shows data of one subject with both sensor gyroscope and accelerometer readings plotted in time domain. Together the sensors offer nine variables.  And the plot has an additional activity class plot. This was repeated to see multiple participants before modeling stage. The following figure shows the output of one subject.
<img src="https://Tahsinmullick.github.io/images/project2_activitylevel_plot.png" alt="project2_activitylevel_plot"></p>
<h6 id="figure-1--timeseries-plot-of-sensor-features-and-activity">Figure 1 : Timeseries plot of sensor features and activity</h6>
<p>The variable plots reveal periods of large movements that match the activity levels 1,2 and 3 that involve walking, walking up and walking down respectively. This provides a good intuition that the variables can be good predictors of the activity levels.</p>
<p>The next thing to inspect was to look into the body acceleration and gyroscopic histograms of subjects. I also plot the total acceleration histogram per activity of a subject. The histogram is plot for each of the 3 axis with a different color x axis -&gt; blue, y-&gt;orange, z-&gt;green. This provided me an understanding of how activities relate to the data.</p>
<p>The first figure in the next page shows the total acceleration histogram per activity. It is observed that activities 1, 2, and 3 show large movement and is some what gaussian in their distribution. For the rest of the activities the distribution is multimodal.</p>
<p>The second figure illustrates body acceleration histogram per activity. This too reaffirms our inference from the first plot. As moving based activities capture Gaussian distribution. This directs us to believe that perhaps acceleration can be a key feature in activity recognition.</p>
<p>The final figure is that of gyroscope. This histogram is mostly Gaussian in the all the activities with very less variance in the stationary activities 4, 5, and 6 and a large variance in 1, 2 and 3.</p>
<p><img src="https://Tahsinmullick.github.io/images/project2_accelerationhistogram_activity_singlesubject.png" alt="project2_accelerationhistogram_activity_singlesubject"></p>
<h6 id="figure-2--total-acceleration-histogram-for-per-activity-of-a-single-subject">Figure 2 : Total acceleration histogram for per activity of a single subject</h6>
<p><img src="https://Tahsinmullick.github.io/images/project2_gyroscope_activity_singlesubject.png" alt="project2_gyroscope_activity_singlesubject"></p>
<h6 id="figure-3--gyroscope-histogram-per-activity-of-a-single-subject">Figure 3 : Gyroscope histogram per activity of a single subject</h6>
<h4 id="baseline-modeling">Baseline modeling:</h4>
<p>As part of baseline modeling, I decided to explore classical machine learning algorithms. In total I explored 5 algorithms with two non-linear algorithms like K-Nearest Neighbor and support vector machines and ensemble algorithms like random forest, bagging decision tree and gradient boosting. The models were evaluated on accuracy and f1 score. The results from the classical ML models are presented in the Table.</p>
<p><img src="https://Tahsinmullick.github.io/images/project2_accuracy_classicalML.png" alt="project2_accuracy_classicalML"></p>
<h4 id="deep-learning-architecture">Deep learning architecture:</h4>
<p>I tested this dataset with two different deep learning architectures. The first is a 1D-CNN as they have shown to perform well on timeseries data. The second is a combination of the 1D-CNN and LSTM model. No hyperparameter tuning was carried out. The models were evaluated on accuracy and F1 score to compare with classical ML techniques.</p>
<p>The CNN model consisted of two 1D convolution layers. The model was also fitted with dropout and 1D Maxpooling. The model ends with two dense layers with Relu and Softmax activation respectively. The optimizer used in the model is Adam and a categorical cross entropy was used for the loss function. The following figures provide the model summary and model architecture.
<img src="https://Tahsinmullick.github.io/images/project2_1dcnn.png" alt="project2_1dcnn"></p>
<h6 id="figure-4--1d-convolutional-neural-network-architecture">Figure 4 : 1D Convolutional neural network architecture</h6>
<p><img src="https://Tahsinmullick.github.io/images/project2_1dcnnlstm.png" alt="project2_1dcnnlstm"></p>
<h6 id="figure-5--1d-convolutional-neural-network-architecture-with-lstm">Figure 5 : 1D Convolutional neural network architecture with LSTM</h6>
<p>The 1D CNN- LSTM combination architecture. This includes CNN layers with LSTM to help in sequenced prediction. The idea is that the CNN will extract features and provide the LSTM as a sequence to model. The modeling in this consists of a single 1D CNN layer that is followed by Maxpooling and then with an LSTM layer. The final layer is a dense layer. The activations used are Rely with Adam optimizer. The evaluation metric was set to accuracy and F1 score. The loss function is a categorical cross entropy. The following figures provide model summary and architecture.</p>
<h4 id="results-and-discussion">Results and Discussion:</h4>
<p>The results reveal that classical ML techniques such as SVM have outperformed the deep learning method in this case with accuracy of 95.04 and F1 score of 0.95. The deep learning models perform well enough to reach good accuracy without any hyperparameter tuning. 1D-CNN model achieves an accuracy of 92.89 and F1 score of 0.90. While the 1D-CNN-LSTM joint model achieves an accuracy of 91.72 and F1 score of 0.91. 
Deep learning models are sensitive to hyperparameters such as kernel size in the case of CNN. It is my understanding that these models can achieve around one or two percent increase if the various hyperparameters are tuned. However, we must determine in such cases whether the use of deep learning for this particular dataset is necessary. The classical ML techniques are doing better than the two tested deep learning models.
<img src="https://Tahsinmullick.github.io/images/project2_dnnaccuracy.png" alt="project2_dnnaccuracy"></p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://Tahsinmullick.github.io/" >
    &copy;  Tahsin Mullick 2022 
  </a>
    <div>
<div class="ananke-socials">
  
    <a href="https://twitter.com/tahsinmullick" target="_blank" class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel="noopener" aria-label="follow on Twitter——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div></div>
  </div>
</footer>

  </body>
</html>
