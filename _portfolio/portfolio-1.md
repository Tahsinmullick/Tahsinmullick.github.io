---
title: "Building Transformers From Scratch: Self-Attention, Multi-Head Attention & Encoderâ€“Decoder Mechanics"
excerpt: "A from-scratch PyTorch implementation of core transformer components including self-attention, masked attention, and multi-head attention following the seminal *Attention Is All You Need* paper.<br/><img src='/images/port_self_attention_vs_mulihead.png'>"
collection: portfolio
---

